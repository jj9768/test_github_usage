% !TEX root = ../intrinsic_offline_RL_bound_arxiv.tex

\section{Introduction}\label{sec:introduction}

In \emph{offline reinforcement learning} (offline RL \cite{levine2020offline,lange2012batch}), the goal is to learn a reward-maximizing policy in an unknown environment (\emph{Markov Decision Process} or MDP) using the historical data coming from a (fixed) behavior policy $\mu$. Unlike online RL, where the agent can keep interacting with the environment and gain new feedback by exploring unvisited state-action space, offline RL usually populates when such online interplays are expensive or even unethical. Due to its nature of without the access to interact with the MDP model (which causes the distributional mismatches), most of the literature that study the sample complexity / provable efficiency of offline RL (\emph{e.g.} \cite{le2019batch,chen2019information,xie2020q,xie2020batch,yin2021near,yin2021nearoptimal,ren2021nearly,rashidinejad2021bridging,xie2021policy}) rely on making different data-coverage assumptions for making the problem learnable and provide the near-optimal worst-case performance bounds that depend on their data-coverage coefficients. Those results are valuable in general as they do not depend on the structure of the particular problem, therefore, remain valid even for pathological MDPs. But is this good enough?

In practice, the empirical performances of offline reinforcement learning (\emph{e.g.} \cite{gulcehre2020rl,fu2020d4rl,fu2021benchmarks,janner2021reinforcement}) are often far better than what those non-adaptive / problem-independent bounds would indicate. Although empirical evidence can help explain why we may observe better or worse performances on different MDPs, a systematic understanding of what types of decision processes and what kinds of behavior policies are inherently easier or more challenging for offline RL is lacking. Besides, despite the fact that a non-adaptive bound can learn even the pathological examples within the assumption family, there is no guarantee for the instances outside the family. However, practical offline reinforcement learning problems are usually beyond the scope of certain data-coverage assumptions, which limits the applicability of those results. Can we make as few assumptions as possible? Or even more, what can we guarantee when no assumption is made about offline learning? 

Those motivate us to derive the provably efficient bounds that are adaptive to the individual instances but only require minimal assumptions so they can be widely applied in most cases. Ideally, such bounds should characterize the system structures of the specific problems, hold even for peculiar instances that do not satisfy the standard data-coverage assumptions, and recover the worst-case guarantees when the assumptions are satisfied. As mentioned in \cite{zanette2019tighter}, a fully adaptive characterization in RL is important as it might bring considerable saving in the time spent designing domain-specific RL solutions and in training a human expert to judge and recognize the complexity of different problems.

\subsection{Our contribution}\label{sec:contribution}


In this work, we provide the analysis for the \emph{adaptive pessimistic value iteration} (APVI) (Algorithm~\ref{alg:APVI}) with finite horizon time-inhomogeneous (non-stationary) MDPs and derive a strong adaptive bound that is near-optimal under the weak assumption $d^\mu_h(s_h,a_h)>0$ if $d^{\pi^\star}_h(s_h,a_h)>0$ (Theorem~\ref{thm:APVI}). Specifically, our bound (quantity \eqref{eqn:intrinsic}) explicitly depends on the marginal importance ratios (between the optimal policy $\pi^\star$ and the behavior policy $\mu$) and the per-step conditional variances. In addition, we provide an instance-dependent (local minimax) lower bound (Theorem~\ref{thm:adaptive_lower_bound}) to certify \eqref{eqn:intrinsic} is nearly optimal at the instance level for offline learning and call it \emph{the intrinsic offline learning bound}. The intrinsic bound has the following consequences.

\begin{itemize}
	\item In the non-adaptive / worst-case regime (\ref{subsec:one}-\ref{subsec:two}), the intrinsic bound implies $\widetilde{O}(H^3/d_m\epsilon^2)$ complexity under the uniform data-coverage \ref{assum:uniform}, $\tilde{O}(H^3SC^\star/\epsilon^2)$ complexity under the single policy concentrability assumption \ref{assum:single_concen} and $\widetilde{O}(H/d_m\epsilon^2)$ complexity when the sum of rewards is bounded by $1$. All of those are optimal in their respectively regimes \citep{yin2021near,rashidinejad2021bridging,xie2021policy,ren2021nearly};
	\item In the adaptive domain (\ref{subsec:three}), the intrinsic bound implies the tight problem-dependent counterpart of \cite{zanette2019tighter}, yields $\tilde{O}(H^3/nd_m)$ fast convergence in the deterministic systems, has improved complexity in the partially deterministic systems and a family of highly mixing problems, and remains optimal when reducing to the tabular contextual bandits.   
\end{itemize}

Beyond the above, due to the generic form of the intrinsic bound, we could come up with as many problem instances (that are of our interests) as possible and study their properties. In this sense, the intrinsic bound helps illuminate the fundamental nature of offline RL.

Furthermore, as a step towards \emph{assumption-free} offline reinforcement learning, we build a {modified} AVPI and obtain an adaptive bound that could characterize the suboptimality gap in the state-action space that is agnostic to the behavior policy (Theorem~\ref{thm:AFRL}). To the best of our knowledge, all of these results are the first of its kinds. 





\subsection{Related work}
Finite sample analysis for offline reinforcement learning can be traced back to \cite{szepesvari2005finite,antos2008fitted,antos2008learning} for the \emph{infinite horizon discounted setting} via Fitted Q-Iteration (FQI) type function approximation algorithms. \citep{chen2019information,le2019batch,xie2020batch,xie2020q} follow this line of research and derive the information-theoretical bounds. Recently, \cite{xie2020batch} considers the offline RL with only the {realizability} assumption, \cite{liu2020provably,chang2021mitigating} considers the offline RL {without sufficient coverage} and \cite{kidambi2020morel,uehara2021pessimistic} uses the model-based approach for addressing offline RL. Under those weak coverage assumption, their finite sample analysis are suboptimal (\emph{e.g.} in terms of the effective horizon $(1-\gamma)^{-1}$). Recently, \cite{yin2021near,yin2021nearoptimal,ren2021nearly} study the finite horizon case. In the linear MDP case, \cite{jin2020pessimism} studies the pessimistic algorithm for offline policy learning under only the compliance assumption, and, concurrently, \cite{xie2021bellman} proposes the general pessimistic function approximation framework with instantiation in linear MDP and \cite{zanette2021provable} shows actor-critic style algorithm is near-optimal for linear Bellman complete model. In addition, \cite{wang2020statistical,zanette2020exponential} prove some exponential lower bounds under their linear function approximation assumptions.


Among them, there are a few works that achieve the sample optimality under their respective assumptions. Under the uniform data coverage (minimal state-action probability $d_m>0$), \cite{yin2021near} first proves the optimal $\tilde{O}(H^3/d_m\epsilon^2)$ complexity in the time-inhomogeneous MDP. Recently, \cite{yin2021nearoptimal} designs the offline variance reduction algorithm to achieve the optimal $\tilde{O}(H^2/d_m\epsilon^2)$ rate for the time-homogeneous case.  Under the setting where the total cumulative reward is bounded by $1$, \cite{ren2021nearly} obtains the horizon-free result with $\tilde{O}(1/d_m)$. More recently, \cite{rashidinejad2021bridging} considers the single concentrability coefficient $C^\star:=\max_{s,a}{d^{\pi^\star}(s,a)}/{d^\mu(s,a)}$ and derives the upper bound $\tilde{O}[(1-\gamma)^{-5}SC^\star/\epsilon^2]$ in the infinite horizon setting which is recently improved by the concurrent work \cite{xie2021policy}. While those worst-case guarantees are desirable, none of them can explain the hardness of the individual problems.\footnote{We do mention \cite{zanette2021provable} is near-optimal in their setting, but it is unclear whether it remains optimal in the standard setting where $Q^\pi\in[0,H]$, since there is an additional $H$ factor by rescaling.}


