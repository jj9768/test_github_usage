\documentclass[11pt]{article}
%\usepackage[letterpaper]{geometry}
\usepackage[in]{fullpage}
\usepackage[parfill]{parskip}
\usepackage{amsmath,amsthm,amssymb,bbm}
\usepackage{mathtools}
\usepackage{cases}
\usepackage{dsfont}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{algorithm,algorithmic}
\usepackage{color}
\usepackage{appendix}
% \usepackage{lmodern}
% \usepackage[lining,semibold]{libertine}
% \usepackage[T1]{fontenc}
% \usepackage[libertine]{newtxmath}
% \usepackage{bm}

\usepackage{bm}
\usepackage{subfigure}
\usepackage{algorithm,algorithmic}
\usepackage{color}
\usepackage{booktabs}       % professional-quality tables
\usepackage{appendix}
\usepackage{authblk}

\usepackage{url}
\usepackage[authoryear]{natbib}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue,linktocpage=true]{hyperref}
\pdfstringdefDisableCommands{\def\Cref#1{#1}}

\usepackage{cleveref}
\crefformat{equation}{(#2#1#3)}
\crefrangeformat{equation}{(#3#1#4) to~(#5#2#6)}
\crefname{equation}{}{}
\Crefname{equation}{}{}


\renewcommand{\qedsymbol}{\hfill\rule{2mm}{2mm}}

\crefname{definition}{\textbf{definition}}{definitions}
\Crefname{definition}{Definition}{Definitions}
\crefname{assumption}{\textbf{assumption}}{assumptions}
\Crefname{assumption}{Assumption}{Assumptions}

% imported defs

%\externaldocument[]{privacy_supp}[privacy_supp.pdf]
\definecolor{maroon}{RGB}{192,80,77}
\definecolor{mypink3}{cmyk}{0, 0.7808, 0.4429, 0.1412}
\newcommand{\maroon}[1]{\textcolor{maroon}{#1}}
\newcommand{\explain}[2]{\underset{\mathclap{\overset{\uparrow}{#2}}}{#1}}
\newcommand{\explainup}[2]{\overset{\mathclap{\underset{\downarrow}{#2}}}{#1}}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
%\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:
\usepackage{amsmath}
\newcommand{\TMIS}{Tabular-MIS estimator}
\newcommand{\SMIS}{State-MIS estimator}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand{\mypink}{\textcolor{mypink3}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\minimize}{\mathop{\mathrm{minimize}}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\defeq}{\mathrel{\mathop:}=}

\usepackage{tikz}
\newcommand*\circled[1]{\scriptsize\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=0.2pt] (char) {#1};}}

\newcommand{\yw}[1]{\textit{\textcolor{red}{[yuxiang]: #1}}} % YW's notes


%\newcommand{\E}{\mathop{\mathbb{E}}}
\def\amdp{\{s,u^\star_t\}}
\def\hoV{\widehat{V}^\star}
\def\hP{\widehat{P}}
\def\tin{\text{in}}
\def\PP{\textit{\textbf{P}}}
\def\hpi{\widehat{\pi}}
\def\E{\mathbb{E}}
\def\P{\mathbb{P}}
\def\Cov{\mathrm{Cov}}
\def\Var{\mathrm{Var}}
\def\half{\frac{1}{2}}
\def\th{\mathrm{th}}
\def\tr{\mathrm{tr}}
\def\df{\mathrm{df}}
\def\dim{\mathrm{dim}}
\def\col{\mathrm{col}}
\def\row{\mathrm{row}}
\def\nul{\mathrm{null}}
\def\rank{\mathrm{rank}}
\def\nuli{\mathrm{nullity}}
\def\sign{\mathrm{sign}}
\def\supp{\mathrm{supp}}
\def\diag{\mathrm{diag}}
\def\aff{\mathrm{aff}}
\def\hy{\hat{y}}
\def\ty{\tilde{y}}
\def\hbeta{\hat{\beta}}
\def\tbeta{\tilde{\beta}}
\def\htheta{\hat{\theta}}
\def\halpha{\hat{\alpha}}
\def\hf{\hat{f}}
\def\lone{1}
\def\ltwo{2}
\def\linf{\infty}
\def\lzero{0}
\def\T{^T}
\def\R{\mathbb{R}}
\def\cA{\mathcal{A}}
\def\cB{\mathcal{B}}
\def\cD{\mathcal{D}}
\def\cE{\mathcal{E}}
\def\cF{\mathcal{F}}
\def\cG{\mathcal{G}}
\def\cH{\mathcal{H}}
\def\cM{\mathcal{M}}
\def\cN{\mathcal{N}}
\def\cP{\mathcal{P}}
\def\cR{\mathcal{R}}
\def\cS{\mathcal{S}}
\def\cT{\mathcal{T}}
\def\cW{\mathcal{W}}
\def\cX{\mathcal{X}}
\def\cY{\mathcal{Y}}
\def\cZ{\mathcal{Z}}
\def\TV{\mathrm{TV}}





\begin{document}

\title{{Towards Instance-Optimal Offline Reinforcement Learning with Pessimism\footnote{To appear at {Conference on Neural Information Processing Systems} (NeurIPS), 2021.}}}

\author[1,2]{Ming Yin }
\author[1]{Yu-Xiang Wang}
\affil[1]{Department of Computer Science, UC Santa Barbara}
\affil[2]{Department of Statistics and Applied Probability, UC Santa Barbara}
\affil[ ]{\texttt{ming\_yin@ucsb.edu}   \quad
	\texttt{yuxiangw@cs.ucsb.edu}}



\date{}

\maketitle

\begin{abstract}
	\input{sections/abstract}
\end{abstract}

\tableofcontents

\input{sections/introduction}

\section{Preliminaries }\label{sec:formulation}
\input{sections/formulation}

\section{A warm-up case study: Vanilla Pessimistic Value Iteration}\label{sec:VPVI}
\input{sections/results}




\section{Sketch of the Analysis}\label{sec:proof_sketch}
\input{sections/proof_overview}

\section{Discussion and Conclusion}\label{sec:conclusion}

This work studies the offline reinforcement learning problem and contributes the intrinsic offline learning bound which is a near-optimal and strong adaptive bound that subsumes existing worst-case bounds under various assumptions. The adaptive characterization of the intrinsic bound abandons the explicit dependence on $H,S,A,C^\star,d_m$ and helps reveal the fundamental hardness of each individual instances. In this sense, it draws a clearer picture of what offline reinforcement learning looks like and serves as a step towards instance optimality in offline RL. 


Nevertheless, it is still unclear whether \eqref{eqn:APVI} is optimal over all the instances. For example, for fully deterministic systems, our bound provides a faster convergence $H^3/n\bar{d}_m$, however, $H^3$ might be very suboptimal comparing to algorithms that are designed specifically for deterministic MDPs, since the agent only need to experience each location $(s,a)$ once to fully acquire the dynamic $P(\cdot|s,a)$ and $r(s,a)$. Recently, \cite{xiao2021optimality} goes beyond the minimax (worst case) optimality and studies the instance optimality behavior for the simplified batch bandit setting. One of their findings is: for ``easy enough'' tasks, different type of algorithms can be equally good, provably. This seems to suggest instance optimality only matters for problems that are hard to learn. How to formally define the instance optimality metric for different problems remains an open problem and how to design a single algorithm that can achieve optimality for all instances could be challenging (or even infeasible). We leave those as the future works.

%Inspiring more problem-dependent structure for online RL?

%Inspiring more correct-structure lower bound for linear MDP?

%Per-instance





\subsection*{Acknowledgment}
The research is partially supported by NSF Awards \#2007117 and \#2003257. MY would like to thank Chenjun Xiao for bringing up a related literature \citep{xiao2021optimality} and Masatoshi Uehara for helpful suggestions.

\bibliographystyle{plainnat}
\bibliography{sections/stat_rl}

\appendix
% \onecolumn

\clearpage
\begin{center}
	 {\LARGE \textbf{Appendix}}
	%{\LARGE Appendix}
\end{center}


\input{sections/proofs}



\end{document}
